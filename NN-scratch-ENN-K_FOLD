predict.dnn <- function(model, data) {
  # new data, transfer to matrix
  new.data <- data.matrix(data)
  # Feed Forwad鍓嶅悜杩愮畻閫氳繃鍓嶅悜浼犳挱绠楁硶寰楀埌鐨勮緭鍑哄眰姣忎釜缁村害鍊间唬琛ㄥ睘浜庤繖涓被鍒殑鍙兘鎬уぇ灏忋€?
  hidden.layer <- sweep(new.data %*% model$W1 ,2, model$b1, '+')#%*%涓や釜鐭╅樀鐩镐箻锛宺璇█涓娇鐢?
  #sweep(x, MARGIN, STATS, FUN="-", ...) 瀵圭煩闃佃繘琛岃繍绠椼€侻ARGIN涓?1锛岃〃绀鸿鐨勬柟鍚戜笂杩涜杩愮畻锛?
  #涓?2琛ㄧず鍒楃殑鏂瑰悜涓婅繍绠椼€係TATS鏄繍绠楃殑鍙傛暟銆侳UN涓鸿繍绠楀嚱鏁帮紝榛樿鏄噺娉曘€倄i*w1+b1 闅愬眰
  # neurons : Rectified Linear
  hidden.layer <- pmax(hidden.layer, 0)#pmax(a,b,c)#姣旇緝a,b,c鍚戦噺涓墍鏈夋暟鍊肩殑澶у皬骞跺皢鏈€澶х殑鍑犱釜鍙栧嚭
  #缁勬垚涓€涓柊鍚戦噺锛屽悜閲忛暱搴︿竴鑸姹備笁涓悜閲忔垨鏄涓悜閲忕浉鍚岋紝濡傛灉涓嶅悓锛屽垯R浼氳嚜鍔ㄥ鐞嗭紝涓€鑸舰鎴愬厓绱?
  #涓暟鏈€澶氱殑銆?
  score <- sweep(hidden.layer %*% model$W2, 2, model$b2, '+')#涓鸿緭鍑哄眰璁＄畻鍒嗘暟
  
  # Loss Function: softmax softmax濡備綍灏嗙缁忕綉缁滃墠鍚戜紶鎾緱鍒扮殑缁撴灉涔熷彉鎴愭鐜囧垎甯冨憿锛烻oftmax鍥炲綊灏辨槸涓€涓潪甯稿父鐢ㄧ殑鏂规硶銆?
  #Softmax鍥炲綊鏈韩鍙互浣滀负涓€涓涔犵畻娉曟潵浼樺寲鍒嗙被缁撴灉锛屽畠鍙槸绁炵粡缃戠粶涓殑涓€灞傞澶栫殑澶勭悊灞傦紝灏嗙缁忕綉缁滅殑杈撳嚭鍙樻垚
  #浜嗕竴涓鐜囧垎甯?
  score.exp <- exp(score)
  probs <-sweep(score.exp, 1, rowSums(score.exp), '/') 
  
  # select max possiblity
  labels.predicted <- max.col(probs)#鎵惧埌姣忎竴琛屼腑鏈€澶у€肩殑浣嶇疆锛岀鍑犲垪
  return(labels.predicted)
}

# Train: build and train a 2-layers neural network 
train.dnn <- function(x, y, traindata, testdata,model = NULL,hidden, maxit,
                      # delta loss 
                      abstol=1e-2,#Stop if the fit criterion falls below abstol, indicating an essentially perfect fit.
                      # learning rate
                      lr = 1e-2,
                      # regularization rate:it's just the additional loss generated by the
                      #regularization function. Add that to the network's loss and optimize over the sum of the two.
                      reg = 1e-3,
                      # show results every 'display' step
                      display = 100,#original is 100
                      random.seed = 1)
{
  # to make the case reproducible.
  set.seed(random.seed)
  
  # total number of training set
  N <- nrow(traindata)
  
  # extract the data and label
  # don't need atribute 
  X <- unname(data.matrix(traindata[,x]))
  # correct categories represented by integer 
  Y <- traindata[,y]
  Y <- as.integer(Y)
   # create index for both row and col
  # create index for both row and col
  Y.len   <- length(unique(Y))
  Y.set   <- sort(unique(Y))
  Y.index <- cbind(1:N, match(Y, Y.set))
  
  # create model or get model from parameter
  if(is.null(model)) {
    # number of input features
    D <- ncol(X)
    # number of categories for classification
    K <- length(unique(Y))
    H <-  hidden
    
    # create and init weights and bias: random generation for the normal distribution with mean
    W1 <- 0.01*matrix(rnorm(D*H), nrow=D, ncol=H)
    b1 <- matrix(0, nrow=1, ncol=H)#bias 鍒濆鍊间负0
    
    W2 <- 0.01*matrix(rnorm(H*K), nrow=H, ncol=K)
    b2 <- matrix(0, nrow=1, ncol=K)
  } else {
    D  <- model$D
    K  <- model$K
    H  <- model$H
    W1 <- model$W1
    b1 <- model$b1
    W2 <- model$W2
    b2 <- model$b2
  }
  
  # use all train data to update weights since it's a small dataset
  batchsize <- N
  # init loss to a very big value
  loss <- 100000
  
  # Training the network
  i <- 0
  
  while(i < maxit && loss > abstol) {
    #i < maxit && loss > abstol
    # iteration index
    i <- i + 1
    
    # forward ....
    # 1 indicate row, 2 indicate col
    hidden.layer <- sweep(X %*% W1 ,2, b1, '+')
    # neurons : ReLU
    hidden.layer <- pmax(hidden.layer, 0)
    score <- sweep(hidden.layer %*% W2, 2, b2, '+')
    
    # softmax
    score.exp <- exp(score)
    # debug
    probs <- score.exp/rowSums(score.exp)
    
    # compute the loss
    corect.logprobs <- -log(probs[Y.index])
    data.loss  <- sum(corect.logprobs)/batchsize
    reg.loss   <- 0.5*reg* (sum(W1*W1) + sum(W2*W2))
    loss <- data.loss + reg.loss
    
    # display results and update model
    if( i %% display == 0) {
      if(!is.null(testdata)) {
        model <- list( D = D,
                       H = H,
                       K = K,
                       # weights and bias
                       W1 = W1, 
                       b1 = b1, 
                       W2 = W2, 
                       b2 = b2)
        labs <- predict.dnn(model, testdata[,-y])
        accuracy <- mean(as.integer(testdata[,y]) == Y.set[labs])
        cat(i, loss, accuracy, "\n")
      } else {
        cat(i, loss, "\n")
      }
    }
    
    # backward ....
    dscores <- probs
    dscores[Y.index] <- dscores[Y.index] - 1
    dscores <- dscores / batchsize
    
    
    dW2 <- t(hidden.layer) %*% dscores 
    db2 <- colSums(dscores)
    
    dhidden <- dscores %*% t(W2)
    dhidden[hidden.layer <= 0] <- 0
    
    dW1 <- t(X) %*% dhidden
    db1 <- colSums(dhidden) 
    
    # update ....
    dW2 <- dW2 + reg*W2
    dW1 <- dW1  + reg*W1
    
    W1 <- W1 - lr * dW1
    b1 <- b1 - lr * db1
    
    W2 <- W2 - lr * dW2
    b2 <- b2 - lr * db2
    
    
    
  }
  
  # final results
  # creat list to store learned parameters
  # you can add more parameters for debug and visualization
  # such as residuals, fitted.values ...
  model <- list( D = D,
                 H = H,
                 K = K,
                 # weights and bias
                 W1= W1, 
                 b1= b1, 
                 W2= W2, 
                 b2= b2)
  
  return(model)
}

########################################################################
# testing
#######################################################################
set.seed(1)

# 0. EDA
dataset <- read.csv("data.csv",header=FALSE,sep=";")
names <- read.csv("names.csv",header=FALSE,sep=";")
names(dataset) <- sapply((1:nrow(names)),function(i) toString(names[i,1]))

library('UBL')
dataset <- ENNClassif( y~., dat=dataset, k = 3, dist = "Euclidean", Cl = c(1,0))
c <- as.data.frame(dataset[[1]])
dataset <- c

resize <- sample(1:3749,1000)
dataset <- dataset[resize,]

k_folds <- 3
folds <- list()

# Split the dataset into a list of data frames where each data frame consists of randomly selected rows from the
# dataset.
# Used https://stats.stackexchange.com/questions/149250/split-data-into-n-equal-groups as a reference
folds <- split(dataset, sample(1:k_folds, nrow(dataset), replace = T))
result_matrix <- matrix(0,nrow = 0,ncol = 3) # Stores the accuracy from each fold

for (q in 1:k_folds)
{
  testingData <- folds[[q]]
  trainingData <- matrix(0, nrow = 0, ncol = 58)
  for (p in 1:k_folds)
  {
    if (p != q)
    {
      trainingData <- rbind(trainingData, folds[[p]])
    }
  }
  # 2. train model
  spam.model <- train.dnn(x=1:57, y=58, traindata=trainingData, testdata=testingData, hidden=50, maxit=20000)
   spam.model
  # 3. prediction
   labels.dnn <- predict.dnn(spam.model, testingData[, -57])
   labels.dnn <- labels.dnn - 1
   # 4. verify the results
   table.dnn<-table(testingData[,58], labels.dnn,dnn=c("actual","predicted"))
   table.dnn 
   #accuracy
  mean(as.integer(testingData[,58]) == labels.dnn)
  acuracy <- 100*(table.dnn[1,1] + table.dnn[2,2]) / sum(table.dnn)
  specifity <- 100*table.dnn[1,1]/(table.dnn[1,2]+table.dnn[1,1]) 
  sensitivity <- 100*table.dnn[2,2]/(table.dnn[2,2]+table.dnn[2,1])
  matrix_once <- matrix(c(acuracy,sensitivity,specifity), nrow=1 , ncol=3)
  result_matrix <- rbind(result_matrix,matrix_once)
  
 }

colnames(result_matrix) <- c("accuracy","sensitivity" , "specifity")
result_average <- c(mean(result_matrix[,1]),mean(result_matrix[,2]),mean(result_matrix[,3]))
result_max <- c(max(result_matrix[,1]),max(result_matrix[,2]),max(result_matrix[,3]))
result_min <- c(min(result_matrix[,1]),min(result_matrix[,2]),min(result_matrix[,3]))
result_deviation <- c(sd(result_matrix[,1]),sd(result_matrix[,2]),sd(result_matrix[,3]))
result_matrix <- rbind(result_matrix, result_average,result_max,result_min,result_deviation)
rownames(result_matrix) <- c(1:k_folds,"ave","max","min","dev")
print(result_matrix)

###########################plot loss against accuracy#############################3
data1 <- ("i loss accuracy
100 0.6234643 0.69 
200 0.5804797 0.73 
300 0.5661959 0.7333333 
400 0.5483803 0.7466667 
500 0.544648 0.7433333 
600 0.535589 0.74 
700 0.5303125 0.75 
800 0.526246 0.7466667 
900 0.5240573 0.75 
1000 0.5218988 0.7533333 
1100 0.5206222 0.75 
1200 0.5192309 0.7466667 
1300 0.5263409 0.73 
1400 0.5169014 0.7266667 
1500 0.5143855 0.7566667 
1600 0.5069467 0.7566667 
1700 0.5196343 0.76 
1800 0.5329266 0.72 
1900 0.5846871 0.7533333 
2000 0.5120396 0.7566667 
2100 0.5157155 0.7266667 
2200 0.510797 0.75 
2300 0.5052584 0.75 
2400 0.5228041 0.7133333 
2500 0.5214404 0.74 
2600 0.529911 0.6966667 
2700 0.5174465 0.7266667 
2800 0.5138235 0.7333333 
2900 0.5049214 0.7533333 
3000 0.5029642 0.7666667 
3100 0.5010476 0.7533333 
3200 0.5280377 0.7 
3300 0.5677581 0.6533333 
3400 0.4945766 0.7466667 
3500 0.4903678 0.7633333 
3600 0.5327935 0.67 
3700 0.498296 0.75 
3800 0.499779 0.7533333 
3900 0.5248961 0.6866667 
4000 0.4974156 0.7433333 
4100 0.4910703 0.7633333 
4200 0.5355958 0.6966667 
4300 0.5884885 0.6233333 
4400 0.5546115 0.6433333 
4500 0.4742767 0.7633333 
4600 0.4927098 0.7566667 
4700 0.4867381 0.7633333 
4800 0.4794284 0.7733333 
4900 0.4850804 0.7766667 
5000 0.4739629 0.7833333 
5100 0.477352 0.78 
5200 0.4738029 0.7666667 
5300 0.5483613 0.68 
5400 0.4770632 0.7433333 
5500 0.4774035 0.7366667 
5600 0.4835767 0.7866667 
5700 0.4946158 0.7333333 
5800 0.4724927 0.7733333 
5900 0.4786384 0.7833333 
6000 0.4743678 0.7933333 
6100 0.496248 0.79 
6200 0.5446058 0.6933333 
6300 0.5706306 0.66 
6400 0.4701753 0.7866667 
6500 0.4702452 0.7966667 
6600 0.5668754 0.6733333 
6700 0.4617907 0.7966667 
6800 0.4697586 0.7933333 
6900 0.5337942 0.7933333 
7000 0.4796081 0.7966667 
7100 0.4616754 0.7733333 
7200 0.4962261 0.7333333 
7300 0.4747096 0.76 
7400 0.5518849 0.6833333 
7500 0.471738 0.7666667 
7600 0.4871498 0.7533333 
7700 0.4800778 0.7733333 
7800 0.4640791 0.7933333 
7900 0.4965234 0.7233333 
8000 0.4652932 0.7833333 
8100 0.459102 0.7933333 
8200 0.4531161 0.8033333 
8300 0.4728384 0.7633333 
8400 0.5275364 0.7133333 
8500 0.5447057 0.69 
8600 0.449831 0.8266667 
8700 0.5299845 0.6933333 
8800 0.5690497 0.6733333 
8900 0.4605421 0.82 
9000 0.4604975 0.8133333 
9100 0.436504 0.8066667 
9200 0.4320141 0.8133333 
9300 0.4277349 0.82 
9400 0.4565005 0.7833333 
9500 0.5450738 0.6866667 
9600 0.4427629 0.8266667 
9700 0.525124 0.7 
9800 0.5203333 0.8166667 
9900 0.4416375 0.7966667 
10000 0.4464942 0.8266667 
10100 0.5285884 0.7133333 
10200 0.6572706 0.6233333 
10300 0.4509027 0.8133333 
10400 0.5357848 0.6866667 
10500 0.433232 0.8066667 
10600 0.6333112 0.62 
10700 0.5128504 0.7166667 
10800 0.4421397 0.8 
10900 0.4819378 0.8233333 
11000 0.4871261 0.8233333 
11100 0.4748078 0.7633333 
11200 0.4882643 0.8266667 
11300 0.4427452 0.83 
11400 0.4328027 0.8266667 
11500 0.5347093 0.6933333 
11600 0.4337847 0.8333333 
11700 0.4294043 0.8333333 
11800 0.6723294 0.65 
11900 0.4582917 0.8133333 
12000 0.4361372 0.8366667 
12100 0.4873062 0.8333333 
12200 0.4422116 0.84 
12300 0.4231742 0.7966667 
12400 0.4539662 0.84 
12500 0.4333149 0.8433333 
12600 0.4377595 0.84 
12700 0.5590157 0.6866667 
12800 0.5029112 0.7366667 
12900 0.5083936 0.7333333 
13000 0.5287753 0.7 
13100 0.5254359 0.7033333 
13200 0.4622466 0.8466667 
13300 0.4432962 0.8466667 
13400 0.5505818 0.6966667 
13500 0.436812 0.8433333 
13600 0.4312039 0.8433333 
13700 0.4252428 0.8433333 
13800 0.4206769 0.8366667 
13900 0.4321942 0.83 
14000 0.4271167 0.83 
14100 0.4744006 0.8333333 
14200 0.4471029 0.84 
14300 0.4325409 0.8133333 
14400 0.5112221 0.73 
14500 0.410201 0.8366667 
14600 0.5614914 0.7 
14700 0.41374 0.83 
14800 0.4487904 0.7733333 
14900 0.4487311 0.8433333 
15000 0.4078673 0.8366667 
15100 0.4808293 0.8433333 
15200 0.436251 0.8433333 
15300 0.484631 0.7533333 
15400 0.5406636 0.7 
15500 0.44192 0.8466667 
15600 0.6033162 0.67 
15700 0.4319374 0.85 
15800 0.4653232 0.7766667 
15900 0.4102047 0.8533333 
16000 0.4155086 0.8433333 
16100 0.4224531 0.8433333 
16200 0.569114 0.6933333 
16300 0.5061706 0.74 
16400 0.6399405 0.65 
16500 0.4061781 0.8533333 
16600 0.4229847 0.8466667 
16700 0.4082473 0.8533333 
16800 0.4435151 0.8433333 
16900 0.4479176 0.7933333 
17000 0.6010165 0.6866667 
17100 0.448339 0.7966667 
17200 0.488968 0.7466667 
17300 0.4205577 0.8433333 
17400 0.395165 0.8533333 
17500 0.4232965 0.8366667 
17600 0.5258105 0.72 
17700 0.5082081 0.7266667 
17800 0.4080475 0.8533333 
17900 0.4552625 0.7866667 
18000 0.4093018 0.86 
18100 0.3985998 0.85 
18200 0.415059 0.8366667 
18300 0.5383042 0.7 
18400 0.4034026 0.8533333 
18500 0.5066143 0.7366667 
18600 0.442692 0.7966667 
18700 0.5235383 0.72 
18800 0.3998016 0.8533333 
18900 0.4603246 0.8366667 
19000 0.4838626 0.76 
19100 0.450397 0.79 
19200 0.4949958 0.7433333 
19300 0.399387 0.8633333 
19400 0.6076734 0.6866667 
19500 0.645147 0.6666667 
19600 0.5561047 0.7033333 
19700 0.5413717 0.6966667 
19800 0.511998 0.74 
19900 0.5612135 0.7 
20000 0.6089559 0.6866667" )

data.v <- read.table(text=data1, header=T)
par(mar=c(5.1, 4.1, 4.1, 4.1))
plot(x=data.v$i, y=data.v$loss, type="o", col="blue", pch=16, 
     main="spam loss and accuracy by 2-layers DNN",
     ylim=c(0, 1.2),
     xlab="",
     ylab="",
     axe =F)
lines(x=data.v$i, y=data.v$accuracy, type="o", col="red", pch=1)
box()
axis(1, at=seq(0,2000,by=200))
axis(4, at=seq(0,1.0,by=0.1))
axis(2, at=seq(0,1.2,by=0.1))
mtext("training step", 1, line=3)
mtext("loss of training set", 2, line=2.5)
mtext("accuracy of testing set", 4, line=2)

legend("bottomleft", 
       legend = c("loss", "accuracy"),
       pch = c(16,1),
       col = c("blue","red"),
       lwd=c(1,1)
)

